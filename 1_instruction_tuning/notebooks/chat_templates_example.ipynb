{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAvFVIAtFlq"
      },
      "source": [
        "# Exploring Chat Templates with SmolLM2\n",
        "\n",
        "This notebook demonstrates how to use chat templates with the `SmolLM2` model. Chat templates help structure interactions between users and AI models, ensuring consistent and contextually appropriate responses."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets trl huggingface_hub"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZveP_78oUs0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-lZu8JvtwUN"
      },
      "outputs": [],
      "source": [
        "# Install the requirements in Google Colab\n",
        "# !pip install transformers datasets trl huggingface_hub\n",
        "\n",
        "# Authenticate to Hugging Face\n",
        "from huggingface_hub import login\n",
        "\n",
        "login()\n",
        "\n",
        "# for convenience you can create an environment variable containing your hub token as HF_TOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnHzBR7vtFlr"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from trl import setup_chat_format\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTVOqbuetFlr"
      },
      "source": [
        "## SmolLM2 Chat Template\n",
        "\n",
        "Let's explore how to use a chat template with the `SmolLM2` model. We'll define a simple conversation and apply the chat template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrxh0oX6tFls",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Dynamically set the device\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name\n",
        ").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
        "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkJwILrbtFls"
      },
      "outputs": [],
      "source": [
        "# Define messages for SmolLM2\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve4dgtjstFls"
      },
      "source": [
        "# Apply chat template without tokenization\n",
        "\n",
        "The tokenizer represents the conversation as a string with special tokens to describe the role of the user and the assistant.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbAg-5x-tFls"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "print(\"Conversation with template:\", input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfvdglOqtFls"
      },
      "source": [
        "# Decode the conversation\n",
        "\n",
        "Note that the conversation is represented as above but with a further assistant message.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXUVdPeytFls"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(\n",
        "    messages, tokenize=True, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcZQpspEtFlt"
      },
      "source": [
        "# Tokenize the conversation\n",
        "\n",
        "Of course, the tokenizer also tokenizes the conversation and special token as ids that relate to the model's vocabulary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc2PLxAMtFlt"
      },
      "outputs": [],
      "source": [
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "print(\"Conversation tokenized:\", input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3eNp9a0tFlt"
      },
      "source": [
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "    <h2 style='margin: 0;color:blue'>Exercise: Process a dataset for SFT</h2>\n",
        "    <p>Take a dataset from the Hugging Face hub and process it for SFT. </p>\n",
        "    <p><b>Difficulty Levels</b></p>\n",
        "    <p>üê¢ Convert the `HuggingFaceTB/smoltalk` dataset into chatml format.</p>\n",
        "    <p>üêï Convert the `openai/gsm8k` dataset into chatml format.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbkXV2_ItFlt",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "display(\n",
        "    HTML(\n",
        "        \"\"\"<iframe\n",
        "            src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n",
        "            frameborder=\"0\"\n",
        "            width=\"100%\"\n",
        "            height=\"640px\"\n",
        "          ></iframe>\n",
        "        \"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")\n",
        "print(ds)\n",
        "for i in range(3):\n",
        "  sample = ds[\"train\"][i]\n",
        "  sample_message = sample[\"messages\"]\n",
        "  print(sample_message)\n",
        "\n",
        "  sample_message_in_template = tokenizer.apply_chat_template(sample_message, tokenize=False, add_generation_prompt=False)\n",
        "  print(sample_message_in_template)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AcpDxFwyZ8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p3atw4_tFlu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def process_dataset(sample):\n",
        "    # TODO: üê¢ Convert the sample into a chat format\n",
        "    # use the tokenizer's method to apply the chat template\n",
        "\n",
        "    sample_message_in_template = tokenizer.apply_chat_template(sample[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
        "    return {\"formatted_chat\": sample_message_in_template}\n",
        "\n",
        "\n",
        "ds = ds.map(process_dataset)\n",
        "print(ds)\n",
        "print(ds[\"train\"][\"formatted_chat\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81fQeazltFlu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "display(\n",
        "    HTML(\n",
        "        \"\"\"<iframe\n",
        "  src=\"https://huggingface.co/datasets/openai/gsm8k/embed/viewer/main/train\"\n",
        "  frameborder=\"0\"\n",
        "  width=\"100%\"\n",
        "  height=\"360px\"\n",
        "></iframe>\n",
        "\"\"\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"openai/gsm8k\", \"main\")\n",
        "print(ds)\n",
        "print(\"=\"*100)\n",
        "for i in range(3):\n",
        "  sample_question = ds[\"train\"][\"question\"][i]\n",
        "  sample_answer = ds[\"train\"][\"answer\"][i]\n",
        "  print(sample_question)\n",
        "  print(sample_answer)\n",
        "  sample_message = [\n",
        "      {\"role\": \"user\", \"content\": sample_question},\n",
        "      {\"role\": \"assistant\", \"content\": sample_answer},\n",
        "  ]\n",
        "  print(sample_message)\n",
        "  print(\"=\"*50)\n",
        "\n",
        "  sample_message_in_template = tokenizer.apply_chat_template(sample_message, tokenize=False, add_generation_prompt=False)\n",
        "  print(sample_message_in_template)\n",
        "  print(\"=\"*100)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gPfmWtHrfFFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bWUSv7NMtFlu"
      },
      "outputs": [],
      "source": [
        "def process_dataset(sample):\n",
        "    # TODO: üêï Convert the sample into a chat format\n",
        "\n",
        "    # 1. create a message format with the role and content\n",
        "\n",
        "    # 2. apply the chat template to the samples using the tokenizer's method\n",
        "    sample_question = sample[\"question\"]\n",
        "    sample_answer = sample[\"answer\"]\n",
        "    sample_message = [\n",
        "        {\"role\": \"user\", \"content\": sample_question},\n",
        "        {\"role\": \"assistant\", \"content\": sample_answer},\n",
        "    ]\n",
        "    sample_message_in_template = tokenizer.apply_chat_template(sample_message, tokenize=False, add_generation_prompt=False)\n",
        "    return {\"formatted_chat\": sample_message_in_template}\n",
        "\n",
        "\n",
        "ds = ds.map(process_dataset)\n",
        "print(ds)\n",
        "print(ds[\"train\"][\"formatted_chat\"][0])\n",
        "print(ds[\"test\"][\"formatted_chat\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXCuRKotFlu"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This notebook demonstrated how to apply chat templates to different models, `SmolLM2`. By structuring interactions with chat templates, we can ensure that AI models provide consistent and contextually relevant responses.\n",
        "\n",
        "In the exercise you tried out converting a dataset into chatml format. Luckily, TRL will do this for you, but it's useful to understand what's going on under the hood."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "py310",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}